# Grafana Alloy Configuration (Unified Observability Agent)
alloy:
  # Cluster identity (used in log/metric labels)
  cluster:
    name: on-prem

  # Simplify resource names (remove redundant 'monitoring-' prefix)
  fullnameOverride: alloy

  # Deploy as DaemonSet to collect logs from all nodes
  controller:
    type: daemonset
    # Define host path volumes for log collection
    volumes:
      extra:
        - name: tetragon-logs
          hostPath:
            path: /var/run/cilium/tetragon
            type: DirectoryOrCreate

  # Disable usage reporting to Grafana Cloud and set configuration
  alloy:
    stabilityLevel: "generally-available"
    extraEnv:
      - name: DISABLE_REPORTING
        value: "true"
    # Expose OTLP receiver ports for trace collection
    extraPorts:
      - name: "otlp-grpc"
        port: 4317
        targetPort: 4317
        protocol: "TCP"
      - name: "otlp-http"
        port: 4318
        targetPort: 4318
        protocol: "TCP"
    # Alloy configuration in River format
    configMap:
      content: |-
        // Discover Kubernetes pods
        discovery.kubernetes "pods" {
          role = "pod"
        }

        // Relabel discovered pods
        discovery.relabel "pods" {
          targets = discovery.kubernetes.pods.targets

          // Add namespace label
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            target_label  = "namespace"
          }

          // Add pod name label
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            target_label  = "pod"
          }

          // Add container name label
          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            target_label  = "container"
          }

          // Add node name label
          rule {
            source_labels = ["__meta_kubernetes_pod_node_name"]
            target_label  = "node"
          }

          // Add cluster label (required by Tetragon dashboards)
          rule {
            target_label  = "cluster"
            replacement   = "{{ .Values.cluster.name }}"
          }

          // Set log path
          rule {
            source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
            separator     = "/"
            target_label  = "__path__"
            replacement   = "/var/log/pods/*$1/*.log"
          }
        }

        // Scrape logs from discovered pods
        loki.source.kubernetes "pods" {
          targets    = discovery.relabel.pods.output
          forward_to = [loki.process.pods.receiver]
        }

        // Process logs (extract container runtime metadata)
        loki.process "pods" {
          forward_to = [otelcol.receiver.loki.to_kafka.receiver]

          stage.cri {}
        }

        // ========================================
        // TETRAGON EVENT LOGS
        // ========================================

        // Discover Tetragon export log file on nodes
        local.file_match "tetragon" {
          path_targets = [{
            __path__ = "/var/run/cilium/tetragon/tetragon.log",
            job      = "tetragon",
            cluster  = "{{ .Values.cluster.name }}",
          }]
        }

        // Read Tetragon export logs
        loki.source.file "tetragon" {
          targets    = local.file_match.tetragon.targets
          forward_to = [loki.process.tetragon.receiver]
        }

        // Process Tetragon logs (they're JSON format)
        loki.process "tetragon" {
          forward_to = [otelcol.receiver.loki.to_kafka.receiver]

          // Parse JSON to extract fields for better querying
          stage.json {
            expressions = {
              process_exec = "process_exec",
              node_name    = "node_name",
            }
          }

          // Add node label from JSON
          stage.labels {
            values = {
              node = "node_name",
            }
          }
        }

        // ========================================
        // LOG PRODUCER PIPELINE (to Kafka)
        // ========================================

        // Convert Loki format logs to OTLP format
        otelcol.receiver.loki "to_kafka" {
          output {
            logs = [otelcol.processor.batch.logs.input]
          }
        }

        // Batch logs before sending to Kafka (improves throughput)
        otelcol.processor.batch "logs" {
          timeout = "5s"
          send_batch_size = 1000
          send_batch_max_size = 2000

          output {
            logs = [otelcol.exporter.kafka.logs.input]
          }
        }

        // Export logs to Kafka using OTLP protocol
        otelcol.exporter.kafka "logs" {
          protocol_version = "2.0.0"

          brokers = ["main-kafka-bootstrap.kafka:9092"]
          topic = "logs-otlp"

          encoding = "otlp_proto"

          producer {
            max_message_bytes = 10000000
            compression = "lz4"
          }

          retry_on_failure {
            enabled = true
            initial_interval = "1s"
            max_interval = "30s"
            max_elapsed_time = "5m"
          }
        }

        // ========================================
        // OTLP TRACE PIPELINE
        // ========================================

        // Receive traces via OTLP (gRPC and HTTP)
        otelcol.receiver.otlp "default" {
          // Listen on standard OTLP ports
          grpc {
            endpoint = "0.0.0.0:4317"
          }

          http {
            endpoint = "0.0.0.0:4318"
          }

          output {
            traces  = [otelcol.processor.batch.default.input]
            // Discard metrics and logs (focus on traces only)
            metrics = []
            logs    = []
          }
        }

        // Batch traces before sending (improves performance)
        otelcol.processor.batch "default" {
          output {
            traces = [otelcol.exporter.otlp.tempo.input]
          }
        }

        // Export traces to Tempo
        otelcol.exporter.otlp "tempo" {
          client {
            endpoint = "tempo:4317"
            // Use insecure connection (internal cluster communication)
            tls {
              insecure = true
            }
          }
        }

    # Resources for Alloy container
    resources:
      requests:
        cpu: 100m  # Increased from 50m - actual usage ~75m (was under-provisioned)
        memory: 256Mi
      limits:
        cpu: 300m
        memory: 576Mi

    # Mount host paths for log collection (Alloy container mounts)
    mounts:
      extra:
        # Tetragon export logs
        - name: tetragon-logs
          mountPath: /var/run/cilium/tetragon
          readOnly: true
