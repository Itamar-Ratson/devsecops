kube-prometheus-stack:
  # Simplify resource names (remove release name prefix)
  # This creates services like: prometheus, alertmanager, kube-state-metrics
  nameOverride: ""

  # Prometheus Configuration (reduced retention/storage for local dev)
  prometheus:
    prometheusSpec:
      retention: 1d
      retentionSize: "1GB"
      # Resources based on metrics (602Mi used, 683Mi max)
      resources:
        requests:
          cpu: 100m
          memory: 700Mi
        limits:
          cpu: "1"
          memory: 1Gi
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 2Gi
      # Automatically discover ServiceMonitors in all namespaces
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false

  # Prometheus Operator Configuration
  prometheusOperator:
    fullnameOverride: prometheus-operator
    # Use deployment-based admission webhooks instead of hook jobs.
    # Hook jobs have a race condition: they complete fast and get deleted
    # before ArgoCD can detect completion, causing sync to hang forever.
    admissionWebhooks:
      # Use cert-manager for TLS certificates
      certManager:
        enabled: true
      # Disable hook-based patching (causes ArgoCD sync hangs)
      patch:
        enabled: false
      # Use persistent deployment instead
      deployment:
        enabled: true
        replicas: 1
        # Resources for the admission webhook deployment
        resources:
          requests:
            cpu: 10m
            memory: 32Mi
          limits:
            cpu: 100m
            memory: 64Mi

  # Kube State Metrics
  kube-state-metrics:
    fullnameOverride: kube-state-metrics

  # Node Exporter
  prometheus-node-exporter:
    fullnameOverride: node-exporter

  # Grafana Configuration
  grafana:
    fullnameOverride: grafana
    enabled: true
    defaultDatasourceEnabled: false
    # Admin credentials from Vault (via VaultStaticSecret -> K8s Secret)
    admin:
      existingSecret: grafana-admin-credentials
      userKey: admin-user
      passwordKey: admin-password
    persistence:
      enabled: true
      size: 1Gi

    # Disable init container that changes ownership (causes permission issues)
    initChownData:
      enabled: false

    # OIDC client secret from Vault
    envFromSecret: grafana-oidc-secret

    # Grafana configuration for Keycloak OAuth
    grafana.ini:
      server:
        root_url: https://grafana.localhost
      auth.generic_oauth:
        enabled: true
        name: Keycloak
        allow_sign_up: true
        client_id: grafana
        # client_secret is injected via envFromSecret as GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET
        scopes: openid profile email groups
        # auth_url uses external URL (browser redirect)
        auth_url: https://keycloak.localhost/realms/devsecops/protocol/openid-connect/auth
        # token_url and api_url use internal URL (server-to-server)
        token_url: http://keycloak-keycloakx-http.keycloak.svc:8080/realms/devsecops/protocol/openid-connect/token
        api_url: http://keycloak-keycloakx-http.keycloak.svc:8080/realms/devsecops/protocol/openid-connect/userinfo
        # Map Keycloak groups to Grafana roles
        role_attribute_path: "contains(groups[*], 'admins') && 'Admin' || contains(groups[*], 'developers') && 'Editor' || 'Viewer'"

    # Sidecar configuration
    sidecar:
      datasources:
        enabled: false
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
        searchNamespace: ALL
        folderAnnotation: grafana_folder
        # Folder annotation for kube-prometheus-stack built-in dashboards
        annotations:
          grafana_folder: "Prometheus Stack"
        provider:
          foldersFromFilesStructure: true

    # Dashboards are provisioned via ConfigMaps (see templates/grafana-dashboards.yaml)
    # The sidecar will automatically load them from ConfigMaps with label grafana_dashboard=1
    # Grafana ingress/service configuration
    service:
      type: ClusterIP
      port: 80

    # Default datasource configuration
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Prometheus
            type: prometheus
            uid: prometheus
            url: http://monitoring-kube-prometheus-prometheus:9090
            access: proxy
            isDefault: true
          - name: AlertManager
            type: alertmanager
            url: http://monitoring-kube-prometheus-alertmanager:9093
            access: proxy
            jsonData:
              implementation: prometheus
          - name: Loki
            type: loki
            uid: loki
            url: http://loki-gateway:80
            access: proxy
            jsonData:
              maxLines: 1000
          - name: Tempo
            type: tempo
            uid: tempo
            url: http://tempo:3200
            access: proxy
            jsonData:
              # Enable trace-to-logs correlation
              tracesToLogsV2:
                datasourceUid: loki
                tags:
                  - key: service.name
                    value: service
                  - key: service.namespace
                    value: namespace
              # Enable trace-to-metrics correlation
              tracesToMetrics:
                datasourceUid: prometheus
                tags:
                  - key: service.name
                    value: service
                  - key: service.namespace
                    value: namespace
              # Enable service map from Prometheus metrics
              serviceMap:
                datasourceUid: prometheus
              # Enable node graph visualization
              nodeGraph:
                enabled: true
              # Enable trace search
              search:
                hide: false

  # AlertManager Configuration (optional, can disable if not needed)
  alertmanager:
    enabled: true
    fullnameOverride: alertmanager
    alertmanagerSpec:
      retention: 24h
      # Mount slack webhook secrets from SealedSecret
      secrets:
        - alertmanager-slack-webhooks
      resources:
        requests:
          cpu: 10m
          memory: 64Mi  # Reduced based on metrics (27Mi used, 30Mi max)
        limits:
          cpu: 100m
          memory: 128Mi
      storage:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 500Mi

    # Alertmanager configuration (routing, inhibition, receivers)
    config:
      global:
        resolve_timeout: 5m

      # Inhibition rules - suppress warnings when critical is firing
      inhibit_rules:
        # Suppress warning when critical exists for same namespace/resource
        - source_matchers:
            - severity = critical
          target_matchers:
            - severity = warning
          equal:
            - namespace
            - resource_name

      # Routing tree
      route:
        group_by: ['alertname', 'namespace', 'severity']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 4h
        receiver: 'default'
        routes:
          # Critical alerts -> Slack #alerts-critical
          - matchers:
              - severity = critical
            receiver: 'slack-critical'
            continue: true
          # Critical alerts -> PagerDuty (receives due to continue: true above)
          - matchers:
              - severity = critical
            receiver: 'pagerduty-critical'
            continue: false
          # Warning alerts -> Slack
          - matchers:
              - severity = warning
            receiver: 'slack-warnings'
            continue: false

      # Receivers
      # Webhook URLs are mounted from SealedSecret via alertmanagerSpec.secrets
      # See seal-slack-webhooks.sh to generate encrypted values
      receivers:
        - name: 'default'
          # Fallback - sends to warnings channel
          slack_configs:
            - api_url_file: /etc/alertmanager/secrets/alertmanager-slack-webhooks/slack-warning-webhook
              channel: '#alerts-warning'
              send_resolved: true
              title: '[{{ .Status | toUpper }}] {{ .CommonLabels.alertname }}'
              text: '{{ .CommonAnnotations.summary }}'

        - name: 'slack-critical'
          slack_configs:
            - api_url_file: /etc/alertmanager/secrets/alertmanager-slack-webhooks/slack-critical-webhook
              channel: '#alerts-critical'
              send_resolved: true
              title: '[{{ .Status | toUpper }}] {{ .CommonLabels.alertname }}'
              text: |-
                *Summary:* {{ .CommonAnnotations.summary }}
                *Description:* {{ .CommonAnnotations.description }}
                *Namespace:* {{ .CommonLabels.namespace }}
                *Resource:* {{ .CommonLabels.resource_name }}

        - name: 'slack-warnings'
          slack_configs:
            - api_url_file: /etc/alertmanager/secrets/alertmanager-slack-webhooks/slack-warning-webhook
              channel: '#alerts-warning'
              send_resolved: true
              title: '[{{ .Status | toUpper }}] {{ .CommonLabels.alertname }}'
              text: |-
                *Summary:* {{ .CommonAnnotations.summary }}
                *Namespace:* {{ .CommonLabels.namespace }}
                *Resource:* {{ .CommonLabels.resource_name }}

        - name: 'pagerduty-critical'
          pagerduty_configs:
            - routing_key_file: /etc/alertmanager/secrets/alertmanager-slack-webhooks/pagerduty-routing-key
              severity: '{{ if eq .Status "firing" }}critical{{ else }}info{{ end }}'
              send_resolved: true
              description: '{{ .CommonAnnotations.summary }}'
              details:
                alertname: '{{ .CommonLabels.alertname }}'
                namespace: '{{ .CommonLabels.namespace }}'
                description: '{{ .CommonAnnotations.description }}'

  # Node Exporter (useful for cluster-level metrics)
  nodeExporter:
    enabled: true

  # Kube State Metrics (Kubernetes cluster metrics)
  kubeStateMetrics:
    enabled: true

  # Default rules (can be customized)
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: false
      configReloaders: true
      general: true
      k8s: true
      kubeApiserver: false
      kubeApiserverAvailability: false
      kubeApiserverSlos: false
      kubelet: true
      kubeProxy: false
      kubePrometheusGeneral: true
      kubePrometheusNodeRecording: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      kubeScheduler: false
      kubeStateMetrics: true
      network: true
      node: true
      nodeExporterAlerting: true
      nodeExporterRecording: true
      prometheus: true
      prometheusOperator: true

